import datetime
import math
import logging
import os
import shutil
import urllib

from contextlib import contextmanager
from decimal import Decimal
from django.conf import settings
from django.core.files import File as PFile
from django.utils import timezone
from django.utils.encoding import smart_str
from zipfile import ZipFile

from .exceptions import NotificationAlreadySent


logger = logging.getLogger(__name__)


def has_unprocessed_files(source_files):
    """
    Returns True if a file is parseable but still has 0 units, and it is not ignored in pricing.
    This can be synonymous to "has unanalyzed" files.
    """
    for source_file in source_files:
        units = source_file.units() or source_file.pages
        if units == 0 and not source_file.ignore_in_pricing() \
            and not source_file.is_unparsable():
            return True
    return False


def has_parsing_error(source_files):
    """
    Returns True if one or more source files has failed parsing
    source_file.status == STATUS_ANALYSIS_FAILED
    """
    return any([source_file.analysis_failed for source_file in iter(source_files)])


def is_analyzing(source_files):
    """
    Source files are still analyzing, returns True if:
    - A file's status is still ``STATUS_ANALYZING``
    - A files still has 0 units.
    """
    has_analyzing = False
    for source_file in source_files:
        if source_file.status == source_file.STATUS_ANALYZING:
            has_analyzing = True
    return has_unprocessed_files(source_files) and has_analyzing


def has_blank_analyzed_files(source_files):
    from liox_file_analyzer.processors.utils import (
        extension_from_filename,
        media_type_from_extension,
    )
    for source_file in source_files:
        extension = extension_from_filename(source_file.original_filename)
        if source_file.is_analyzed and source_file.units() == 0 \
           and media_type_from_extension(extension):
            return True
    return False


def has_analyzed_files_with_units(source_files):
    for source_file in iter(source_files):
        if not source_file.is_analyzed and source_file.units() <= 0:
            return False
    return True


def has_all_target_files(job, source_files=None, target_locales=None):
    """
    Returns true if there is a target file for all of the target locales

    If no source files or target locales are passed all of those belonging to
    the job will be used.

    Args:
        - job (object): od_app.models.Job
        - source_files (Optional[list/tuple/queryset]): contains File object
            with file_type='source'
        - target_locales (Optional[list/tuple/queryset]): contains Locale object
            from od_app.models.Locale
    """
    from models import File

    if source_files is None:
        source_files = job.source_files.all()

    if target_locales is None:
        target_locales = job.target_locales.all()

    for source_file in source_files:
        for locale in target_locales:
            try:
                File.objects.get(origin=source_file, language=locale)
            except File.DoesNotExist:
                return False
            except File.MultipleObjectsReturned:
                # Looks like we have more than one target file.  That is OK.
                pass

    return True


def has_all_target_products(job, source_products=None, target_locales=None):
    """
    Returns true if there is a target product for all of the target locales

    If no source products or target locales are passed all of those belonging
    to the job will be used.

    Args:
        - job (object): od_app.models.Job
        - source_product (Optional[list/tuple/queryset]): contains Product object
            with product_type='source'
        - target_locales (Optional[list/tuple/queryset]): contains Locale object from
            od_app.models.Locale
    """
    from .models import Product

    if source_products is None:
        source_products = job.source_products.all()

    if target_locales is None:
        target_locales = job.target_locales.all()

    for source_product in source_products:
        for locale in target_locales:
            try:
                Product.objects.get(origin=source_product, language_code=locale.code)
            except Product.DoesNotExist:
                return False
            except Product.MultipleObjectsReturned:
                pass

    return True


def calculate_cost_for_units(service, source_locale, target_locales, units, org=None):
    """
    Basic Equation: Total Units of all Source Files * Service Unit Cost
    """
    cost = _decimals(units * service.as_usd('per_unit_rate'))

    for target_locale in target_locales:
        penalty_value = cost_multiplier(source_locale, target_locale, org=org)
        cost += penalty_value * _decimals(units) * _decimals(service.as_usd('per_language_rate'), 3)

    return _decimals(cost, 3)


def multiplier(source_locale, target_locale, penalty_type):
    from models import PenaltyMatrix

    # Updated this one to be compatible with data migration
    try:
        query = {
            'source_locale': source_locale,
            'target_locale__in': target_locale.all(),
        }
    except AttributeError:
        if isinstance(target_locale, list):
            query = {
                'source_locale': source_locale,
                'target_locale__in': target_locale,
            }
        else:
            query = {
                'source_locale': source_locale,
                'target_locale': target_locale,
            }

    penalty_value = float(max(
        PenaltyMatrix.objects.filter(
            **query
        ).values_list(penalty_type, flat=True)
        or [0]
    ))/100

    return _decimals(penalty_value + 1)


def cost_multiplier(source_locale, target_locale, org=None):
    if org and org.price_penalty_exempt:
        return 1
    return multiplier(source_locale, target_locale, 'price_penalty')


def time_multiplier(source_locale, target_locale):
    return multiplier(source_locale, target_locale, 'time_penalty')


def get_total_units(service, source_files, source_products, unit_type_override=''):
    """
    Gets all of the units in collection of source files and products
    If unit_type != words, these values will be the same.
    Returns a dictionary with keys:
     - `total_raw_units`: Returns raw unit counts.
     - `total_effort_units`: Returns leveraged unit counts.
    """

    total_raw_units = 0

    unit_type = unit_type_override or service.pricing_unit_type
    if source_files:
        # Returns the file source types
        for file_obj in source_files:
            if file_obj.extension in service.ignore_in_pricing_list:
                continue
            total_raw_units += file_obj.units(unit_type)

    if source_products:
        # Returns the product source types
        for product_obj in source_products:
            total_raw_units += product_obj.units(unit_type)

    if unit_type_override:
        # If unit type is overriden, there should be no leverage
        total_effort_units = 0
    else:
        total_effort_units = total_raw_units

    return {
        'total_units': total_raw_units,
        'effort_units': total_effort_units
    }


def get_file_bytes(source_files):
    total = 0
    for file_obj in source_files:
        try:
            total += file_obj.file_size
        except TypeError:
            # probably means that i don't have an attachment. bad data
            pass
    return total


def has_unprocessed_files_and_parsing_error(source_files):
    """
    Optimized for checking if has_unprocessed_files and
    has_parsing_error to just iterate once with the two results
    """
    has_unprocessed_files = False
    has_parsing_error = False

    for source_file in source_files:
        units = source_file.units() or source_file.pages
        if units == 0 and not source_file.ignore_in_pricing() \
            and not source_file.is_unparsable():
            has_unprocessed_files = True
        if source_file.analysis_failed:
            has_parsing_error = True

        # break the loop if has_unprocessed_files and has_parsing_error
        # to avoid iterating remaining files
        if has_unprocessed_files and has_parsing_error:
            break

    return has_unprocessed_files, has_parsing_error


def trunc(invalue, digits):
    return int(invalue*math.pow(10,digits))/math.pow(10,digits);


def _decimals(value, decimal_places=2):
    """
    Converts to Decimal object with the default of 2 decimal places
    """
    decimal_string = str(trunc(float(value), decimal_places))
    return Decimal(decimal_string)


def clear_job_targets(job):
    """
    remove target files/products of job
    Used in:
    - reset_job_target_assets
    """
    for _file in job.target_files:
        if _file.attach:
            try:
                _file.attach.delete()
            except AttributeError:
                #this probably means that the file
                # doesn't really exist
                pass
        _file.delete()

    if job.delivery:
        try:
            job.delivery.delete()
        except AttributeError:
            #this probably means that the file
            # doesn't really exist
            pass

    for product in job.target_products:
        product.delete()

    job.completion_notification_date = None
    job.save()


def reset_job_target_assets(job):
    """
    - Removes all of a job.target_files, deletes all job.target_products and
      delete its job.delivery.
    - Removes all job.translation.zip_file and reverts all job.translation status.
    """
    clear_job_targets(job)

    for translation in job.translations.all():
        # Revert the translation's status until it is "NEW"
        while not translation.is_new:
            translation._revert_current_status()
            translation.save()


def create_initial_job(service, site, quote, **kwargs):
    """
    For source=WEB, responsible for creating the job instance on step-1
    """
    from .models import Job
    # Local timezone specified by ip_address on middleware
    tz_name = timezone.get_current_timezone_name()

    create_kwargs = {
        'service': service,
        'creation_date': datetime.datetime.utcnow().replace(tzinfo=timezone.utc),
        'site': site,
        'submitted_timezone': tz_name,
    }
    create_kwargs.update(kwargs)

    if quote.owner:
        create_kwargs.update({'owner': quote.owner})

    job_instance = Job.objects.create(**create_kwargs)

    # Assign default_source_locale if job_instance has no source_locale
    if not getattr(job_instance, 'source_locale') and quote.owner:
        job_instance.source_locale = quote.owner.profile.default_source_lang
        job_instance.save()

    quote.jobs.add(job_instance)

    return job_instance


def get_target_files_zip(target_files):
    """
    When a project is complete create a target file zip with a root folder called "target_files"
    and a subdirectory for each target language (using the language code).

    Save zip files to /tmp temporarily

    Accepts a queryset of File objects

    Returns the zip file path
    """
    from liox_on_demand.od_app.utils import retrieve_file_from_url

    job = target_files.first().job

    if job.unbundleable:
        logger.info('Job.unbundleable is True, id: %s', job.id)
        return

    if target_files.oversized() or target_files.oversized(actual=True):
        job.unbundleable = True
        job.save()
        logger.info('NOT creating zip for target files for completed Job: %s since it is oversized', job.id)
        return

    logger.info('Creating zip for target files for completed job #%d', job.id)

    # Set temp names for zip file and work dir
    temp_file_name = 'job_%s_target_files.zip' % job.id
    temp_dir_name = 'job_%s_target_upload' % job.id

    # Set temp paths for zip file and work dir
    temp_zip_file_path = os.path.join(
        settings.TMP_DIR,
        temp_file_name
    )
    temp_work_dir_path = os.path.join(
        settings.TMP_DIR,
        temp_dir_name
    )

    # Remove temp zip file and work dir if they already exist
    if os.path.isfile(temp_zip_file_path):
        os.remove(temp_zip_file_path)
    if os.path.exists(temp_work_dir_path):
        shutil.rmtree(temp_work_dir_path)

    # Create temp work dir
    os.makedirs(temp_work_dir_path)

    # Get target files from s3 and save to temp work dir
    # After saving, zip target files
    
    try:
        with ZipFile(temp_zip_file_path, 'w', allowZip64=True) as my_zip:
            for target_file in target_files:
                # Set target file paths
                original_filename = target_file.original_filename
                target_file_path = os.path.join(
                    temp_work_dir_path,
                    original_filename
                )

                # Get target file from s3 and save to temp work dir
                if target_file.attach:
                    retrieve_file_from_url(target_file.attach.url, target_file_path)

                # Set subdirectory for target file
                subdirectory = target_file.language.code
                target_file_name = original_filename
                archive_name = os.path.join(
                    subdirectory,
                    target_file_name
                )
                # Add target file to zip
                my_zip.write(
                    target_file_path,
                    archive_name
                )
        # Remove temp work dir
        shutil.rmtree(temp_work_dir_path)

        # Return temp zip file path
        return temp_zip_file_path

    except Exception as e:
        # Create dict with the Exception Errors
        exception = {e.__class__.__name__: e.message}
        e.message = {'job utils method': 'get_target_files_zip'}
        e.message.update(exception)
        raise e


def check_job_valid_for_delivery(job, target_files, raise_error=False):
    """
    This function checks if job is valid for creating delivery file
    """
    def handle_failure(message):
        if raise_error:
            raise Exception(message)
        else:
            logger.info(message)
            return False

    if not job.is_complete:
        return handle_failure('Job #%s is not yet complete' % job.id)
    elif not job.file_based:
        return handle_failure('Job #%s is not file based' % job.id)
    elif job.delivery:
        return handle_failure('Job #%s has completed target files zip' % job.id)
    elif job.unbundleable:
        return handle_failure('Job #%s has oversized target files' % job.id)
    # check if target files is not empty
    if not target_files:
        return handle_failure('Job #%s has no target files' % job.id)

    missing_attach_targets = []
    for target in target_files:
        if not target.attach:
            missing_attach_targets.append(str(target.pk))

    if missing_attach_targets:
        target_pks = ', '.join(missing_attach_targets)
        return handle_failure('Job #%s target files without attach. [%s]' % (job.id, target_pks))

    return True


def generate_target_files_zip(target_files, raise_error=False):
    """
    Generates target files zip for completed jobs
    This function is used from command `generate_target_files_zip`
    """
    job = target_files.first().job
    if not check_job_valid_for_delivery(job, target_files, raise_error):
        return False
    try:
        # Save to job.delivery
        file_path = get_target_files_zip(target_files)
    except Exception:
        logger.exception(u"Error retrieving target files for job %d", job.id)
    else:
        if file_path is None:
            logger.info('Delivery will not be available for job: %s because it is too large', job.id)
            return False
        with open(file_path, 'rb') as file_:
            file_to_save = PFile(file_)
            job.delivery.save('target_files', file_to_save)
    finally:
        os.remove(file_path)
    return True


def get_estimated_completion(job, creation_date=None):
    """
    Returns estimated job completion date which is calculated based on ops tz.
    """
    if not creation_date:
        creation_date = timezone.now()

    return job.get_due_date(started_date=creation_date, tz='ops')


def create_translation_zip(translation):
    """
    Creates a zip file for the translation specified using all the target_files
    for the job/translation query.
    """
    if translation.unbundleable:
        return

    job = translation.job
    locale_code = translation.locale.code
    target_files = job.target_files.filter(language=translation.locale)

    if target_files.oversized() or target_files.oversized(actual=True):
        translation.unbundleable = True
        translation.save()
        logger.info('NOT creating translation zip for Job: %s, translation: %s since it is oversized', job.id, locale_code)
        return

    temp_file_name = 'job_%s_translation_%s.zip' % (job.id, locale_code)
    temp_dir_name = 'job_%s_translation_%s' % (job.id, locale_code)
    temp_zip_file = os.path.join(settings.TMP_DIR, temp_file_name)
    temp_work_dir_path = os.path.join(settings.TMP_DIR, temp_dir_name)

    # Remove temp zip file and work dir if they already exist
    if os.path.isfile(temp_zip_file):
        os.remove(temp_zip_file)
    if os.path.exists(temp_work_dir_path):
        shutil.rmtree(temp_work_dir_path)

    # Create temp work dir
    if not os.path.exists(temp_work_dir_path):
        os.makedirs(temp_work_dir_path)

    with ZipFile(temp_zip_file, 'w', allowZip64=True) as tmp_zip:
        for target_file in target_files:
            original_filename = target_file.original_filename
            target_file_path = os.path.join(temp_work_dir_path, original_filename)
            urllib.urlretrieve(target_file.attach.url, target_file_path)
            tmp_zip.write(target_file_path, os.path.join(locale_code, original_filename))

    # Save zip into translation.zip_file
    zip_file = PFile(open(temp_zip_file, 'rb'))
    translation.zip_file.save('%s_translations.zip' % locale_code, zip_file)
    translation.save()
    # Remove temp file and temp dir
    os.remove(temp_zip_file)
    shutil.rmtree(temp_work_dir_path)


def create_job_reference_files_zip(job_id):
    """
    Creates a zip file for the field reference_files_zip for the
    specified job.

    Include source_files when job.service.source_files_as_reference is True
    """
    logger.info('Starting create_job_reference_files_zip for job: #%s', job_id)
    from .models import File, Job
    from .utils import get_work_dir, retrieve_file_from_url

    try:
        job = Job.objects.get(id=job_id)
    except Job.DoesNotExist:
        logger.warning('Job ID: %s is invalid.', job_id)
        return

    if job.reference_files_zip:
        logger.warning('Job ID: %s has reference_files_zip already.', job_id)
        return

    temp_work_dir = get_work_dir(settings.TMP_DIR, 'job_%s_%s_reference_files_work_dir' % (job.pk, job.quote.pk))
    zip_temp_file = os.path.join(temp_work_dir, 'job_%s_reference_files.zip' % job.pk)

    # List of dictionary for reference_files
    # [{'temp_name': 'temp.txt', {'orig_name': 'orig.txt'}}]
    reference_files = []

    if job.service.source_files_as_reference:
        logger.debug('Creating reference zip with reference files and source files for Job #%s', job.id)
        files_to_be_zipped = job.reference_or_source_files
    else:
        logger.debug('Creating reference zip with reference files for Job #%s', job.id)
        files_to_be_zipped = job.reference_files.all()

    for file_obj in files_to_be_zipped:
        logger.info('Downloading file #%s, type: %s', file_obj.id, file_obj.get_file_type_display())
        temp_name = os.path.join(temp_work_dir, smart_str(file_obj.original_filename))

        retrieve_file_from_url(file_obj.attach.url, temp_name)

        reference_files.append({
            'temp_name': temp_name,
            'orig_name': file_obj.original_filename,
            'type': file_obj.file_type
        })

    with ZipFile(zip_temp_file, 'w', allowZip64=True) as myzip:
        for ref_file in reference_files:
            logger.info('Adding file #%s to zip', ref_file['orig_name'])
            if ref_file['type'] == File.FILE_TYPE_SOURCE:
                myzip.write(ref_file['temp_name'], os.path.join('source', smart_str(ref_file['orig_name'])))
            else:
                myzip.write(ref_file['temp_name'], smart_str(ref_file['orig_name']))

    with open(zip_temp_file, 'rb') as zip_sock:
        logger.info('Saving reference_files_zip %s', zip_temp_file)
        job.reference_files_zip = PFile(zip_sock)
        job.save()

    shutil.rmtree(temp_work_dir)


def add_jobs_and_send_notifications(quote):
    from .tasks import send_new_project_notifications, celery_task_add_job
    """
    For the given job id, do the following:
        - Send mail and sync to Marketo
        - Send to tms
    """
    # Task to to send emails and sync Marketo
    send_new_project_notifications.apply_async(args=[quote.id])

    for job_id in quote.jobs.values_list('id', flat=True):
        # Task to send to TMS
        celery_task_add_job.apply_async(args=[job_id])


@contextmanager
def safe_notify(job):
    """
    Used as a context manager for sending job completion notifications.

    Raise an exception if notification has already been sent. Otherwise it
    runs the code block and then stores a notification date.

    If you want to raise an exception when notification should not be
    sent, simply do this::

        with safe_notify(job):
            customer_email = JobSendMail(job, email_type='complete')
            customer_email.send_email()

    If you want subsequent code to run, you can do this::

        my code ...
        try:
            with safe_notify(job):
                customer_email = JobSendMail(job, email_type='complete')
                customer_email.send_email()
        except NotificationAlreadySent:
            pass
        my other code ...
    """
    if job.completion_notification_date:
        raise NotificationAlreadySent('Completion notification for Job %s has already been sent' % job.id)
    yield
    job.completion_notification_date = timezone.now()
    job.save(update_fields=['completion_notification_date'])
